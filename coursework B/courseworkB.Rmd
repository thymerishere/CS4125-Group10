---
title: "Coursework B"
author: "..."
output: html_notebook
---



```{r}
d <- read.csv("data.csv")
str(d)
```

```{r}
table(d$model, d$TeD)
```

```{r}
d$modeltype = factor(substring(d$model, 1, 1))
d$modelnr = factor(substring(d$model, 2))
temp = subset(d, select=c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8"))
d$trdcount = rowSums(temp, dims=1)
d$model = factor(d$model)
```

```{r}
d
```

```{r fig.width=7, fig.height=3}
library(ggplot2)

ggplot(d[d$modeltype == "B",], aes(x=score, y=modelnr)) + geom_point(aes(col=TeD), alpha=0.5, size=5) + ggtitle("Baseline models")
```

```{r fig.width=7, fig.height=2}
ggplot(d[d$modeltype == "S",], aes(x=score, fill=TeD)) + geom_histogram() + ggtitle("Single TrD model")
```

```{r fig.width=7, fig.height=10}
ggplot(d[d$modeltype == "M",], aes(x=score, fill=TeD)) + geom_histogram() + facet_grid(modelnr ~ .) + ggtitle("Multiple TrD models")
```

```{r fig.width=7, fig.height=4}
ggplot(d, aes(x=trdcount, y=score)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2, alpha=0.3, size=1, aes(col=modeltype))
```

# Conceptual model

...

# Statistical tests

## Research question 1

*How much does transfer learning improve over typical non-transfer learning?*

Data analysis

Visualizing score distribution across TeDs.
``` {r}
p <- ggplot(dtask, aes(x=score, fill=TeD)) + geom_density(position = "stack", size=0.1) + facet_grid(~modeltype)
ggsave("score_density.pdf", width=5, height=3)
p
```
Visualizing score distribution across tasks.
``` {r}
dtask <- data.frame(d)
my_map <- c("TeD1"="Classification", "TeD2"="Classification", "TeD3"="Classification", "TeD4"="Classification", "TeD5"="Recommendation", "TeD6"="Regression", "TeD7"="Regression")
dtask$task <- my_map[dtask$TeD]
ggplot(dtask, aes(x=score, fill=task)) + geom_density(position = "stack", size=0.1) + facet_grid(~modeltype)
ggsave("scores_task_density.pdf", width=6,height=3)
```
Visualizing score distributions of only the transfer learning models across tasks.
``` {r}
dtl <- d[which((d$modeltype == "M" | d$modeltype == "S") & d$model != "B1" & d$model != "B2" & d$TeD != "TeD5"), c("model", "TeD", "score")]
ggplot(dtl, aes(y=score, fill=model)) + geom_density(position = "stack", size=0.1) + facet_grid(~TeD)
ggsave("scores_per_ted_only_tf.pdf", width=10, height=4)
```
``` {r}
dtl <- d[which((d$modeltype == "M" | d$modeltype == "S") & d$model != "B1" & d$model != "B2" & d$TeD == "TeD5"), c("model", "TeD", "score")]
ggplot(dtl, aes(x=score, fill=model)) + geom_density(position = "stack", size=0.1) + facet_grid(~TeD)
ggsave("scores_per_ted_only_tf_only_ted5.pdf", height=2, width=3)
```

First we evaluate if there is a difference between the means of the baseline model scores and the non-baseline (transfer learning) model scores.
To evaluate whether the test dataset is of influence on whether transfer learning improves we set up a linear mixed effects model.

```{r}
library(nlme)

model0 <- lme(score ~ 1 , random = ~1|TeD, data = d, method="ML")
model1 <- lme(score ~ modeltype , random = ~1|TeD, data = d, method="ML")
summary(model0)
intervals(model0, 0.95)
summary(model1)
intervals(model1, 0.95)
pander(anova(model0, model1), caption = "Model comparison.")
```
First we analyse whether transfer learning achieves a better mean score on all test sets. Results show a significant score increase (p < 0.01) of 0.118 when transfer learning is used.
``` {r}
library(dplyr)
library(rio)
library(emmeans)
library(pander)

m <- lm(score ~ modeltype*TeD, d)
pairs(emmeans(m, ~modeltype | TeD))
```

<!-- Nog wat toevoegen over p value ajustment (wat wordt gedaan door emmeans) -->
Next we will analyse the mean scores of the transfer learning models. Comparing the mean scores of S and M on the test sets shows that using additional training sets significantly increases (p < 0.01) the mean score. Comparing the mean score of MN and M1 shows that refining one part of the model on the target test set significantly increases (p < 0.01) the score over performing no refinement. The differences between different amounts of refinement are small. Doing refinement on the full model yields a significantly higher mean score (p < 0.01) over refining one and two parts, but is not significantly better (p = 0.05) than doing refinement on three parts of the model only.
``` {r}
dtl <- d[which(d$modeltype == "Transfer Learning" & d$model != "B1" & d$model != "B2"), c("model", "TeD", "score")]
m <- lm(score ~ model*TeD, dtl)
em <- emmeans(m, ~model)
plot(em)
ggsave("emmeans_refinement.pdf")
pairs(em)
```
``` {r}
dtl <- d[which(d$modeltype == "Transfer Learning" & d$model != "B1" & d$model != "B2"), c("model", "TeD", "score")]
dtl$singleTrain <-factor(dtl$model == "S",labels=c("Two or More Training Sets", "Single Training Set"))

m <- lm(score ~ singleTrain*TeD, dtl)
em <- emmeans(m, ~singleTrain)
plot(em)
ggsave("single_train.pdf")
pairs(em)
```

We will now investigate the influence of refinement of the models on the mean scores on the different test sets. From the boxplots it looks like refinement increases the mean score on all test data sets, although on TeD1 the median has decreased with full model refinement.
``` {r, fig.width=10, fig.height=4}
library(ggplot2)
dtlr <- d[which(d$model == "MN" | d$model == "M1" | d$model == "M2" | d$model == "M3" | d$model == "MF" ), c("model", "TeD", "score")]
dtlr$model <- factor(dtlr$model, levels=c("MN", "M1", "M2", "M3", "MF"))
ggplot(dtlr, aes(model, score)) + geom_boxplot() + facet_grid(~TeD)
ggsave("boxplots_refinement.pdf", plot=last_plot())
```
Carrying out pairwise comparisons of each model while faceting by test set shows that on all test sets except TeD5 there is a significant improvement (p < 0.01) in doing any level of refinement. However, refining more than one component of the model gives only significantly better results (p < 0.05) on some of the test sets (TeD1, TeD2, TeD4, TeD7).
``` {r}
m <- lm(score ~ model*TeD, dtlr)
pairs(emmeans(m, ~model | TeD))
```


## Researh question 2

*What is the effect of the TrDâ€™s on the final model performance?*

```{r}
cols <- c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8")
d[cols] <- lapply(d[cols], factor)
d$trdcount = factor(d$trdcount)
```

```{r}
library(nlme)
model0 <- lme(score ~ 1, random = ~1|TeD, data = d, method="ML")
model1 <- lme(score ~ TrD1 + TrD2 + TrD3 + TrD4 + TrD5 + TrD6 + TrD7 + TrD8, random = ~1|TeD, data = d, method="ML")
model2 <- lme(score ~ trdcount, random = ~1|TeD, data = d, method="ML")
```

```{r}
library(jtools)

summary(model0)
intervals(model0, 0.95)
plot(model0)
plot(model1)
```

```{r}
summary(model1)
intervals(model1, 0.95)
plot(model1)
```
```{r}
summary(model2)
intervals(model2, 0.95)
plot(model2)
```

```{r}
library(pander)
pander(anova(model0, model1), caption = "Model comparison between model 0 and model 1")
pander(anova(model1, model2), caption = "Model comparison between model 0 and model 1")
```

```{r fig.width=7, fig.height=4}
ggplot(d, aes(x=trdcount, y=score)) + geom_boxplot()
```
```{r}
emmeans(model2, ~trdcount)
```


```{r}
require(lme4)
model3 <- glmer(score ~ trdcount + (1 | TeD), data=d)
model2 <- glme(score ~ trdcount + (1 | TeD), data = d)
pander(anova(model2, model3), caption = "Model comparison between model 2 and model 3")
summary(model3)
intervals(model3, 0.95)
plot(model3)
```

